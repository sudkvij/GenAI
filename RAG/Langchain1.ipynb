{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudkvij/GenAI/blob/main/RAG/Langchain1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4c98aeb-8e99-4791-b3b2-1b213f93dc2e",
      "metadata": {
        "id": "b4c98aeb-8e99-4791-b3b2-1b213f93dc2e"
      },
      "source": [
        "LLM Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90040a2e-4750-4c9d-8321-627a2585ad53",
      "metadata": {
        "id": "90040a2e-4750-4c9d-8321-627a2585ad53"
      },
      "outputs": [],
      "source": [
        "#from langchain_openai import ChatOpenAI\n",
        "#lim = ChatOpenAI(openai_api_key=\"\")\n",
        "from langchain_community.llms import Ollama\n",
        "llm = Ollama(model=\"llama2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4527321a-f94c-441d-8394-57ac091a8a52",
      "metadata": {
        "id": "4527321a-f94c-441d-8394-57ac091a8a52",
        "outputId": "f9ea8fd8-8df2-48bb-9ce2-dd70301abc60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nLangsmith is a tool that can help with testing in several ways:\\n\\n1. **Code coverage analysis**: Langsmith can analyze the test code and provide insights into the code coverage, which can help identify areas of the code that are not being tested enough or at all.\\n2. **Test case generation**: Langsmith can generate test cases based on the existing codebase, which can help developers write more comprehensive tests and reduce the number of false negatives.\\n3. **Regression testing**: Langsmith can be used to identify changes in the codebase that may affect the functionality of the application, allowing developers to prioritize regression testing efforts.\\n4. **Test data generation**: Langsmith can generate test data based on the existing data, which can help developers write more comprehensive tests and reduce the number of false negatives.\\n5. **Test automation**: Langsmith can be used to automate testing tasks, such as running tests, analyzing test results, and identifying defects.\\n6. **Continuous Integration/Continuous Deployment (CI/CD)**: Langsmith can be integrated into CI/CD pipelines to help identify issues early in the development cycle and reduce the time it takes to deploy changes to production.\\n7. **Testing for security**: Langsmith can be used to identify potential security vulnerabilities in the codebase, such as SQL injection or cross-site scripting (XSS) attacks.\\n8. **Testing for performance**: Langsmith can be used to identify areas of the code that may impact the performance of the application, such as slow database queries or bottlenecks in the application logic.\\n9. **Testing for accessibility**: Langsmith can be used to identify potential accessibility issues in the codebase, such as lack of alt text for images or inaccessible navigation.\\n10. **Testing for compliance**: Langsmith can be used to identify potential compliance issues in the codebase, such as GDPR or HIPAA requirements.\\n\\nBy using Langsmith for testing, developers can save time and resources by automating repetitive tasks, improving test coverage, and identifying potential issues early in the development cycle.'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.invoke(\"how can langsmith help with testing?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04adaeba-cc2c-4bbb-b536-ea385868478d",
      "metadata": {
        "id": "04adaeba-cc2c-4bbb-b536-ea385868478d"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are world class technical documentation writer.\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de6f747a-4f98-44e9-a5f9-ca1e86526992",
      "metadata": {
        "id": "de6f747a-4f98-44e9-a5f9-ca1e86526992"
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa4b2526-434a-44fa-9325-212332520f3d",
      "metadata": {
        "id": "fa4b2526-434a-44fa-9325-212332520f3d",
        "outputId": "56e1f6eb-c3a5-4875-98a3-b074524d950e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nAh, an excellent question! As a world-class technical documentation writer, I must say that Langsmith is a versatile tool that can greatly assist in the testing process. Here are some ways Langsmith can help:\\n\\n1. Automated Testing: Langsmith's AI-powered engine can automatically generate test cases based on your technical documentation. This can save you time and resources by reducing the need for manual testing, allowing your team to focus on other critical tasks.\\n2. Consistency Checking: Langsmith's advanced algorithms can analyze your documentation to identify inconsistencies and errors. By catching these issues early on, you can avoid potential problems down the line and ensure that your technical documents are accurate and up-to-date.\\n3. Content Completeness Analysis: Langsmith can evaluate the completeness of your technical content by analyzing factors such as the inclusion of all necessary information, the clarity of explanations, and the accuracy of references. This can help you identify areas where you need to provide more detail or clarify confusing concepts.\\n4. Grammar and Spell Checking: Langsmith's built-in grammar and spell checker can help you catch errors in your technical writing that may have gone unnoticed during the drafting process. This can improve the overall quality of your documentation and enhance its credibility.\\n5. Style Guide Compliance: By analyzing your documentation against a predefined style guide, Langsmith can help ensure consistency in formatting, layout, and language usage throughout your technical content. This can greatly improve the readability and professionalism of your documents.\\n6. Documentation Organization: Langsmith's advanced algorithms can help you organize your technical documentation in a logical and intuitive manner, making it easier for readers to find the information they need quickly and efficiently.\\n7. Metrics Analysis: Langsmith can provide valuable insights into the usage and effectiveness of your technical documentation, such as which sections are most frequently accessed or how users navigate through your content. This can help you identify areas where you can improve the user experience and optimize your documentation for maximum impact.\\n\\nIn summary, Langsmith is a powerful tool that can greatly assist in the testing process by automating certain tasks, improving consistency and completeness, catching errors, and providing valuable insights into usage patterns. As a world-class technical documentation writer, I highly recommend leveraging Langsmith's capabilities to enhance the quality and efficiency of your technical writing efforts!\""
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e58b1307-0da5-4f76-b00c-8545d5282aea",
      "metadata": {
        "id": "e58b1307-0da5-4f76-b00c-8545d5282aea"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "734c7284-a70b-458b-ba4a-9ed5fe8f9ce3",
      "metadata": {
        "id": "734c7284-a70b-458b-ba4a-9ed5fe8f9ce3"
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d3f761f-0125-4c7c-9d82-ffd25fefee6c",
      "metadata": {
        "id": "9d3f761f-0125-4c7c-9d82-ffd25fefee6c",
        "outputId": "598130ad-5583-4c91-ce27-9b5a86a0f385"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nAs a world-class technical documentation writer, I'm glad you asked! Langsmith is more than just a language model - it's a versatile tool that can assist in various aspects of software testing, including documentation. Here are some ways Langsmith can help with testing:\\n\\n1. Automated Documentation Generation: Langsmith can automatically generate technical documentation for your software, saving you time and effort. With the ability to process large amounts of information, Langsmith can quickly create comprehensive documentation, including API references, user manuals, and more.\\n2. Consistency and Accuracy: Langsmith's advanced language models ensure that your documentation is consistent and accurate across different parts of your software. This can help reduce errors and improve the overall quality of your documentation.\\n3. Customization: Langsmith allows you to customize your documentation to fit your specific needs. You can create tailored documentation for different audiences, such as developers, managers, or end-users, and even generate documentation in multiple languages.\\n4. Testing Support: Langsmith can assist in testing by generating test cases based on the documentation it generates. This can help identify potential issues earlier in the development process, reducing the risk of errors and improving overall quality.\\n5. Documentation Review: Langsmith's language models can review your documentation to identify any inconsistencies or errors, ensuring that your documentation is accurate and up-to-date.\\n6. Collaborative Workflow: Langsmith enables collaboration with other developers and stakeholders by providing a platform for real-time feedback and reviews. This can help ensure that everyone is on the same page and working towards the same goal.\\n7. Knowledge Management: Langsmith's advanced language models can be trained to recognize and extract information from your software documentation, making it easier to maintain and update documentation over time. This can help improve knowledge management within your organization and reduce the risk of knowledge loss.\\n\\nBy leveraging these features, Langsmith can significantly improve the efficiency and accuracy of your technical documentation, helping you deliver high-quality software products to your customers faster and more efficiently.\""
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc4b80e7-1e3f-494a-821e-551415f047ed",
      "metadata": {
        "id": "cc4b80e7-1e3f-494a-821e-551415f047ed"
      },
      "source": [
        "Retrieval Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c21ddfda-9a21-4dcb-bb06-700e68fe70c0",
      "metadata": {
        "id": "c21ddfda-9a21-4dcb-bb06-700e68fe70c0"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c78c0480-99cb-4c76-84a6-e39f57957e34",
      "metadata": {
        "id": "c78c0480-99cb-4c76-84a6-e39f57957e34"
      },
      "outputs": [],
      "source": [
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "embeddings = OllamaEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46d3bbc0-bfdb-4b3a-83e7-7796f01205c6",
      "metadata": {
        "id": "46d3bbc0-bfdb-4b3a-83e7-7796f01205c6"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter()\n",
        "documents = text_splitter.split_documents(docs)\n",
        "vector = FAISS.from_documents(documents, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afaad19d-5bf8-4367-a9b8-136d0ea872a0",
      "metadata": {
        "id": "afaad19d-5bf8-4367-a9b8-136d0ea872a0"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "Question: {input}\"\"\")\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b86456d4-5520-448e-b14c-028ec47fee1a",
      "metadata": {
        "id": "b86456d4-5520-448e-b14c-028ec47fee1a",
        "outputId": "cf4dda71-7e48-4a69-e3c7-4dd4a3be0360"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, langsmith can help with testing by allowing users to visualize their test results. This means that users can use langsmith to create visual representations of their test data, such as graphs or charts, in order to better understand and analyze their test results. This can be particularly useful for identifying trends or patterns in the data, and for making informed decisions based on the test results.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.documents import Document\n",
        "document_chain.invoke({\n",
        "    \"input\": \"how can langsmith help with testing?\",\n",
        "    \"context\": [Document(page_content=\"langsmith can let you visualize test results\")]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f854741-5802-4d12-b3df-1c10c6462505",
      "metadata": {
        "id": "2f854741-5802-4d12-b3df-1c10c6462505"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "retriever = vector.as_retriever()\n",
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc310184-1ee2-4057-ad98-cb965178065a",
      "metadata": {
        "id": "dc310184-1ee2-4057-ad98-cb965178065a",
        "outputId": "e1bcb86f-6557-4646-ef50-582766f2b4b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangSmith can help with testing in several ways:\n",
            "\n",
            "1. **Evaluating prompts**: LangSmith provides automatic evaluation metrics for LLMs, which can help identify areas where the prompt needs to be improved.\n",
            "2. **Testing chains**: LangSmith allows you to test changes to a chain by running it over a dataset and visualizing the outputs. This helps identify any issues with the chain and make adjustments as needed.\n",
            "3. **Collecting examples**: LangSmith provides an \"Add to Dataset\" button for each run, allowing you to easily collect examples of inputs and outputs for testing purposes.\n",
            "4. **Collaborative debugging**: LangSmith enables collaboration in debugging by providing a \"Share\" button that allows you to share the chain and LLM runs with colleagues for debugging.\n",
            "5. **Tracing**: LangSmith's tracing feature helps identify the sequence of calls and inputs/outputs of each call, which can help identify potential issues in the chain.\n",
            "6. **Monitoring failures**: LangSmith allows you to monitor failures and add examples of failed runs to a chosen dataset, which can help identify potential issues with the chain.\n"
          ]
        }
      ],
      "source": [
        "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
        "print(response[\"answer\"])\n",
        "\n",
        "# LangSmith offers several features that can help with testing:..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53ab682e-ab8c-4735-ad44-b6b51e4f4e1d",
      "metadata": {
        "id": "53ab682e-ab8c-4735-ad44-b6b51e4f4e1d"
      },
      "source": [
        "Conversation Retrieval chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb02a8ac-80a3-438f-a831-273cb50195d6",
      "metadata": {
        "id": "fb02a8ac-80a3-438f-a831-273cb50195d6"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "# First we need a prompt that we can pass into an LLM to generate this search query\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
        "])\n",
        "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edf24767-6a27-4a04-8b8a-bbd50d6b0225",
      "metadata": {
        "id": "edf24767-6a27-4a04-8b8a-bbd50d6b0225",
        "outputId": "eecafd8f-8536-41c3-832c-6d8c9baec0e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat weâ€™ve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If weâ€™re being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluationâ€‹Automatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
              " Document(page_content=\"types, chains, agents, and retrievers in the future.What is the exact sequence of events?â€‹In complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being made? In what order? What are the inputs and outputs of each call?LangSmith's built-in tracing feature offers a visualization to clarify these sequences. This tool is invaluable for understanding intricate and lengthy chains and agents. For chains, it can shed light on the sequence of calls and how they interact. For agents, where the sequence of calls is non-deterministic, it helps visualize the specific sequence for a given run -- something that is impossible to know ahead of time.Why did my chain take much longer than expected?â€‹If a chain takes longer than expected, you need to identify the cause. By tracking the latency of each step, LangSmith lets you identify and possibly eliminate the slowest components.How many tokens were used?â€‹Building and prototyping LLM applications can be expensive. LangSmith tracks the total token usage for a chain and the token usage of each step. This makes it easy to identify potentially costly parts of the chain.Collaborative debuggingâ€‹In the past, sharing a faulty chain with a colleague for debugging was challenging when performed locally. With LangSmith, we've added a â€œShareâ€� button that makes the chain and LLM runs accessible to anyone with the shared link.Collecting examplesâ€‹Most of the time we go to debug, it's because something bad or unexpected outcome has happened in our application. These failures are valuable data points! By identifying how our chain can fail and monitoring these failures, we can test future chain versions against these known issues.Why is this so impactful? When building LLM applications, itâ€™s often common to start without a dataset of any kind. This is part of the power of LLMs! They are amazing zero-shot learners, making it possible to get started as easily as possible. But this can also be a curse -- as you adjust the prompt, you're wandering blind. You donâ€™t have any examples to benchmark your changes against.LangSmith addresses this problem by including an â€œAdd to Datasetâ€� button for each run, making it easy to add the input/output examples a chosen dataset. You can edit the example before adding them to the dataset to include the expected result, which is particularly useful for bad examples.This feature is available at every step of a nested chain, enabling you to add examples for an end-to-end chain, an intermediary chain (such as a LLM Chain), or simply the LLM or Chat Model.End-to-end chain examples are excellent for testing the overall flow, while single, modular LLM Chain or LLM/Chat Model examples can be beneficial for testing the simplest and most directly modifiable components.Testing & evaluationâ€‹Initially, we do most of our evaluation manually and ad hoc. We pass in different\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
              " Document(page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoringâ€‹After all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.Weâ€™ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing â€” mirroring the debug mode approach.Weâ€™ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasetsâ€‹LangSmith makes it easy to curate datasets. However, these arenâ€™t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.â†©PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright Â© 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
              " Document(page_content='LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
        "retriever_chain.invoke({\n",
        "    \"chat_history\": chat_history,\n",
        "    \"input\": \"Tell me how\"\n",
        "})"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}